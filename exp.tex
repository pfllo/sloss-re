\section{Experiments}

\subsection{Experimental Settings}
\paragraph{Datasets}
%Describe the two datasets.
We evaluate our approach on two datasets, including one English datasets and one Chinese dataset.

The English dataset, DBpedia dataset, is the one generated by \cite{chen2014encoding}, by mapping the triples in DBpedia \cite{bizer2009dbpedia} to the sentences in New York Time corpus. It has 51 different relations, includes about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 sentences for testing.

The Chinese dataset, HudongBaiKe dataset, is also generated by \cite{chen2014encoding}, they derive knowledge facts and construct a Chinese KB from the Infoboxes of HudongBaiKe, one of the largest Chinese online encyclopedias. They collect four national economic newspapers in 2009 as their corpus. 28 different relations are mapped to the corpus and this results in 60,000 entity tuples, 120,000 sentences for training and 40,000 tuples, 83,000 sentences for testing.

\paragraph{Hyperparameters}
%Describe the hyper parameters.
We use a grid search to determine the optimal parameters, and the parameter setting used in the final model is listed in Table 2.

\begin{table}[!t]  
	\centering  
	\scriptsize  
	\caption{Parameter settings}  
	\begin{tabular}{ll}  
		\\[-2mm]  
		\hline  
		\hline\\[-2mm]
		\vspace{1mm} 
		CNN Window size      &   \tabincell{l}{3}\\  
		\vspace{1mm}  
		Sentence embedding size       &   \tabincell{l}{256}\\  
		\vspace{1mm}  
		English Word dimension  &   \tabincell{l}{50}\\  
		\vspace{1mm}  
		Chinese Word dimension  &   \tabincell{l}{300}\\  
		\vspace{1mm}  
		Position dimension       &   \tabincell{l}{5}\\  
		\vspace{1mm}  
		Batch size        &   \tabincell{l}{50}\\
		\vspace{1mm}  
		Learning rate       &   \tabincell{l}{0.001}\\
		\vspace{1mm}  
		Semantic loss rate        &   \tabincell{l}{0.001}\\
		\hline  
		\hline  
	\end{tabular} 
	
	\label{tab:notations}  
\end{table} 


\subsection{Experimental Results}
%We evaluate PR-curve (briefly introduce the PR-curve) for SL, base model, ILP, simplified SL.
Following previous works, we use the Precision-Recall curve as the evaluation criterion in out experiment. To prove our approach make sense, we compare out results with baseline CNN model \cite{lin2016neural} and ILP method \cite{chen2014encoding}, and we also show the results of Simplified Semantic Loss.
\paragraph{Compare with Base Model}
Compare the semantic loss method and the base model (clear improvement).

\paragraph{Compare with ILP Method}
Compare the semantic loss method and ILP.

NOTE: clw also use cardinality constraints

\paragraph{Compare with Simplified Semantic Loss}
Compare the semantic loss method and the simplified semantic loss.
Both on PR-curve, and on time.




