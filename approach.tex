\section{Our Approach}
%Briefly introduce the whole framework, and refer to each subsection one by one.
In this section, we describe a novel loss term that incorporates the argument type and cardinality constraints required by each relation,
which significantly increases the quality of relation extraction results with only moderate extra cost during training.
Before delving into the details of our method (Sec.~\ref{sec:loss_term}), we will first briefly introduce our base relation extraction model (Sec.~\ref{sec:base_model}), and the constraints that we will use as well (Sec.~\ref{sec:constraints}).





\subsection{Base Model}
\label{sec:base_model}
%Briefly introduce the base model (PCNN).
Although our loss term is compatible to most existing relation extractors, in this paper, we take the attentive piecewise convolutional neural network (\APCNN)~\cite{lin2016neural} as our base model, which is currently one of the most widely used extractor in the relation extraction task.

Specifically, \APCNN operates in the \MIL framework that takes in all the sentences mentioning the target subject and object, and output the relations between these two entities.
We first use the piecewise convolutional neural network (\PCNN)~\cite{zeng2015distant} to obtain the embedding of each sentence.
Then, an attention layer is applied to these sentence embeddings to selectively aggregate them into a sentence bag embedding, which is then fed to a softmax classifier to generate the predicted relation distribution, $\textbf{p}$.

%First, we concatenate the word embeddings and position embeddings of each word in one sentence, the position embeddings indicates how close each word is to head or tail entities. Then, we use a convolutional layer with window size 3, a max pooling layer and a non-linear layer to merge all these features and get the representation of one sentence.Then a selective attention is done over a bag of sentences to get the bag representation.


\subsection{Relation Constraints}
\label{sec:constraints}
%Describe clw's argument type constraints
%只介绍两类规则，一类是不同关系主体客体的约束，一类是相同关系主体客体的约束

%\cite{xu2017semantic}  encodes the symbolic knowledge in Boolean logic, and derives a semantic loss to make the neural network automatically learn the symbolic knowledge. Their experiments show that the semantic loss helps to reduce the constraint violation and gives significant practical improvements in semi-supervised classification. 

%Inspired by their work, we encode the constraints in \cite{chen2014encoding} to boolean logic, and add a semantic loss to the neural network.

Following Chen et al.~\shortcite{chen2014encoding}, we derive type and cardinality constraints from an existing KB to implicitly capture the expected type and cardinality requirements for the arguments of a relation. 
%\paragraph{Implicit Argument Types Inconsistencies:}
\paragraph{Type Constraint}
%Due to the lack of well-defined typing systems in most \KBs, determining the expected argument type of each relation with appropriate granularity is usually a hard problem.
%Generally, the argument types of the correct predictions should be consistent with each other.
Similar to Chen et al.~\shortcite{chen2014encoding}, we also use entity sharing between relations to implicitly capture the expected argument type of each relation.
Specifically, if the subject set of relation $R_1$ in \KB has a large intersection with those in $R_2$, then we consider $R_1$ and $R_2$ have the same expected subject type. 
We thereby collect relation paris that have the same subject type ($\textbf{C}^{ts}$), object type ($\textbf{C}^{to}$), and relation pairs whose subject type of $R_1$ is the same as the object type of $R_2$ ($\textbf{C}^{tso}$).
%finding relation pairs that can share the same subject ($C^s$), object ($C^o$), and those where the subject of one relation can be the object of the other ($C^{so}$).
Then, $\textbf{C}^{ts}$, $\textbf{C}^{to}$ and $\textbf{C}^{tso}$ represents the constraints that we expect ($r_1$, $r_2$) of the predicted ($subj_1$, $r_1$, $obj_1$),  ($subj_2$, $r_2$, $obj_2$) pairs fall into $\textbf{C}^{ts}$ when $subj_1=subj_2$, $\textbf{C}^{to}$ when $obj_1=obj_2$ and $\textbf{C}^{tso}$ when when $subj_1=obj_2$ respectively.
%Then, we expect the relations of all pairs of predicted \emph{(subject, relation, object)} triples to fall into $\textbf{C}^{ts}$, $\textbf{C}^{to}$ or $\textbf{C}^{tso}$ when they share the same entity in either subject or object.
The idea is that, if the predicted relations of two triples require the same entity to belong to different types, then at least one of the prediction must be wrong.
%The idea is that, take $C^s$ for example, if the subject set of relation $R_1$ has a large intersection with those in $R_2$ (($R_1$, $R_2$) lies in $C^s$ ), then the expected subject type of $R_1$ and $R_2$ are likely to be the same. Therefore, if a pair of predicted relation $R_1'$ and $R_2'$ for two entity pairs share the same subject, which implies $R_1'$ and $R_2'$ have the same expected subject type, then ($R_1'$, $R_2'$) must lie in $C^s$. 
%\red{luo: Hard to explain this clearly given limited space. Maybe we can delete the intuition part.}

% if the predicted relations of two entity pairs require the same entity to belong to mutually-exclusive types, then at least one of the prediction must be wrong.
%Take \textless USA, New York\textgreater and \textless USA, Washington D.C.\textgreater as an example. We couldn't predict two relations for them which has different Subject types, such as $ LargestCity $ and $ LocationCity $.

%\paragraph{Violations of Arguments' Uniqueness:}
\paragraph{Cardinality Constraint}
Given a subject (or object), some relations should only have one object (or subject).
For example, the relation \emph{Capital} would expect only one object for a given subject.
Following this observation, we collect relations that can have multiple objects ($\textbf{C}^{co}$) or subjects ($\textbf{C}^{cs}$),
and $\textbf{C}^{co}$ (or $\textbf{C}^{cs}$) represents the constraint that we expect the relation of the predicted ($subj_1$, $r$, $obj_1$),  ($subj_2$, $r$, $obj_2$) pairs fall into $\textbf{C}^{co}$ (or $\textbf{C}^{cs}$) when $subj_1=subj_2 \land obj_1\neq obj_2$ (or $subj_1\neq subj_2 \land obj_1=obj_2$)
%expect the predicted relations that have multiple objects (or subjects) for a given subject (or object) to fall into $\textbf{C}^{co}$ (or $\textbf{C}^{cs}$).

% For example, given USA as the subject of the relation Capital, we can only accept one possible object, because there is great chance that a country only have one capital.

\iffalse
\begin{table}[!t]  
	\centering  
	\scriptsize  
	\begin{tabular}{ll}  
		\\[-2mm]  
		\hline  
		\hline\\[-2mm]
		\vspace{1mm}
		%\multicolumn{2}{|c|}{?}\\
		Details about Argument Type Constraints in \cite{chen2014encoding}\\
		\hline\\[-2mm]  
		\vspace{1mm}
		Given two entity tuples $(s_1, r_1, o_1), (s_2, r_2, o_2)$:\\ 
		\vspace{1mm} 
		Subj-Cons      &   \tabincell{l}{$s_1 \neq s_2 \quad when \quad r_1 \neq r_2$}\\  
		\vspace{1mm}  
		Obj-Cons       &   \tabincell{l}{$o_1 \neq o_2 \quad when \quad r_1 \neq r_2$}\\  
		\vspace{1mm}  
		Subj-Obj-Cons  &   \tabincell{l}{$s_1 \neq o_2 \vee o_1 \neq s_2 \quad when \quad r_1 \neq r_2$}\\  
		\vspace{1mm}  
		Subj-Uni       &   \tabincell{l}{$s_1 \neq s_2 \quad when \quad r_1 = r_2 \wedge o_1 = o_2$}\\  
		\vspace{1mm}  
		Obj-Uni        &   \tabincell{l}{$o_1 \neq o_2 \quad when \quad r_1 = r_2 \wedge s_1 = s_2$}\\  
		\hline  
		\hline  
	\end{tabular} 
	\caption{Argument Type Constraints}  
	\label{tab:notations}  
\end{table} 
\fi
\iffalse
Given two entity tuples $(s_1, r_1, o_1), (s_2, r_2, o_2)$: 

Subj-Cons:$s_1 \neq s_2 \quad when \quad r_1 \neq r_2$
	
Obj-Cons:$o_1 \neq o_2 \quad when \quad r_1 \neq r_2$
	
Subj-Obj-Cons:$s_1 \neq o_2 \vee o_1 \neq s_2 \quad when \quad r_1 \neq r_2$

Subj-Uni:$s_1 \neq s_2 \quad when \quad r_1 = r_2 \wedge o_1 = o_2$

Obj-Uni:$o_1 \neq o_2 \quad when \quad r_1 = r_2 \wedge s_1 = s_2$
\fi
%Subj-Cons: $r_1$ and $r_2$ can't have same subject. Obj-Cons means $r_1$ and $r_2$ can't have same object. Subj-Obj-Cons means $r_1$'s subject can't be $r_2$'s object or $r_2$'s subject can't be $r_1$'s object. Subj-Uni means if $r_1$ equals $r_2$, 


\subsection{Incorporating  Constraints for Training}
\label{sec:loss_term}
In this section, we demonstrate our method of converting the relation constraints into a loss term using the semantic loss framework~\cite{xu2017semantic}.
We also make suitable modifications to speed up the training process with only minor drop in prediction ability.

\paragraph{Relation Constraint Loss}
%Describe semantic loss, and how to encode clw's constraints into semantic loss.
Semantic loss is a general framework that can encode a propositional logic constraints as a loss term in a principled way.
%and derive a semantic loss to make the neural network automatically learning the symbolic knowledge. 
%This loss function captures how close the neural network is to satisfying the constraints on its output. 
Concretely, the semantic loss $L^{s}(\textbf{C}, \textbf{p})$ is defined as:
%For example, the semantic loss between an exactly-one constraint $\alpha$ and a neural net output vector $p$ captures how close the prediction $p$ is to having exactly one output set to true(1), and all false(0), regardless of which output is correct.
%We desire our semantic loss $L^{s}(\alpha, p)$ to be proportional to the negative log-probability of $\alpha$ being satisfied when sampling values according to $p$. More formally,
%In \cite{xu2017semantic}, the semantic loss function is formed as follows,
\begin{center}
	$L^{s}(\textbf{C}, \textbf{p}) = -log\sum\limits_{\bm x\models\textbf{C}}\prod\limits_{i:\bm x\models X_i}p_i\prod\limits_{i:\bm x\models \neg X_i}(1-p_i)$
\end{center}
%over variables $\bm X=\{X_1,...,X_n\}$, and a vector of probabilities $p$, where $p_i$ denotes the predicted probability of variable $X_i$: 
where $\textbf{C}$ is the set of propositional logic constraints defined over variables $\textbf{X}$,
$p_i$ is the predicted probability of $X_i$,
$\bm x \models \textbf{C}$ refers to the assignment $\bm x$ of variables $\bm X$ that satisfies constraints in $\textbf{C}$,
and $i:\bm x \models X_i$ (or $i:\bm x\models \neg X_i$) refers to all the indices $i$ where $X_i$ is set to true (or false) in assignment $\bm x$.

As for our type constraints, the variables $\textbf{X}\in \{0,1\}^{2R}$, where $R$ is the number of relations, are defined over a pair of  relations ($r_1$, $r_2$) which are assigned to 2 entity pairs ($subj_1$, $obj_1$) and ($subj_2$, $obj_2$) respectively.
Specifically, $X_i\in \textbf{X}_{1...R}$ equals 1 only when $r_1$ is the $i^{th}$ relation,
and $X_{R+i}\in \textbf{X}_{R+1...2R}$ equals 1 only when $r_2$ is the $i^{th}$ relation.
We thereby define three semantic loss terms: $m^{ts}L^{s}(\textbf{C}^{ts}, \textbf{p})$, $m^{to}L^{s}(\textbf{C}^{to}, \textbf{p})$, $m^{tso}L^{s}(\textbf{C}^{tso}, \textbf{p})$, where $m^{ts}$, $m^{to}$, $m^{tso}$ are 0-1 masks that are set to 1 when $subj_1=subj_2$, $obj_1=obj_2$, $subj_1=obj_2$ respectively.

%means that $X_i$ is set to true in world $\bm x$. 
%Intuitively, this is the self-information of obtaining an assignment that satisfies the constraint. 
%Intuitively, utilizing this loss pushes up the likelihood of assignments $\bm x$ that satisfies sentence $\alpha$.

As for our cardinality constraints, the variables $\textbf{X}\in \{0,1\}^{R}$ are defined over a pair of a relations $r$ that is assigned to 2 entity pairs ($subj_1$, $obj_1$) and ($subj_2$, $obj_2$) simultaneously.
Specifically, $X_i\in \textbf{X}$ equals 1 only when $r$ is the $i^{th}$ relation,
We thereby define two semantic loss terms: $m^{co}L^{s}(\textbf{C}^{co}, \textbf{p})$ and $m^{cs}L^{s}(\textbf{C}^{cs}, \textbf{p})$, where $m^{co}$ and $m^{cs}$are 0-1 masks that are set to 1 when $subj_1=subj_2 \land obj_1\neq obj_2$ and $subj_1\neq subj_2 \land obj_1=obj_2$ respectively.

Note that, for each loss term, all the relation assignment pairs that satisfiest the corresponding constraints are included.
Therefore, minimizing these semantic loss terms actually increases the likelihood of all the relation predictions that satisfies the type constraints.


%To utilize \cite{chen2014encoding}'s constraints into our neural net, we transform these constraints into boolean logic just like \cite{xu2017semantic}. For example, (Subj, Obj, Subj-Obj, rel1, rel2) = (1, 0, 0, Nationality, Birthplace) is a constraint in \cite{chen2014encoding}, limiting the argument type of relations, which means that two relation $ Nationality $ and $ Birthplace $ could have the same Subject , different Object and one relation's Subject is different with other's Object. Suppose there are 5 kinds of relations: rel1, rel2=$ Nationality $, rel3, rel4=$ Birthplace $, rel5, we encode this rule with a boolean vector (1, 0, 0, 0, 1, 0, 1, 0), the first three position indicates the (Subj, Obj, Subj-Obj) information, and the last five position indicates which two relations involved in the rule. And all the boolean form of constraints in \cite{chen2014encoding} composed of  $ \alpha $ in (1).

\paragraph{Training Procedure}
%How do you construct the batches?
%What is the loss function?
Here we introduce the learning and optimization details of our model. Our objective function is consists of two parts: cross-entropy loss and semantic loss.
\begin{center}
	$ J(\theta) = J_{en}(\theta) + J_{SL}(\theta)$
\end{center}

The cross-entropy loss defined as follows:
\begin{center}
	$ J_{en}(\theta)= \sum\limits_{i=1}^{s}logp(r_i|S_i, \theta)$	
\end{center}
where $ s $ indicates the number of sentence bags and $ \theta $ indicates all parameters of our model.

The semantic loss defined as follows:

For any combination of two entity pair in a batch, we use the CNN output on them to get the probability vector $ p $ in (1). Then we use (1) to calculate $L^{s}(\alpha, p)$ on all combinations of two entity pair. So we can finally get semantic loss as follows:
\begin{center}
	$ J_{SL}(\theta) = \sum\limits_{i \neq j}L^{s}(\alpha, p_{ij}) $,
\end{center}
where $0<=i<=batch-size, 0<=j<=batch-size$, $ p_{ij} $ means the probability vector calculate by $ O_i, O_j $

To solve the optimization problem, we adopt Adam \cite{kingma2014adam} to minimize the objective function. For learning, we iterate by randomly selecting a mini-batch from the training set until converge.
\paragraph{Simplified Semantic Loss}

%Describe the simplified version of the semantic loss.
In our experiments, we find that the way \cite{xu2017semantic} calculating semantic loss is time-consuming, so we simplify the semantic loss calculation without losing much performance, which greatly reduces the time overhead.

In (1), we need calculate semantic loss for every $ \bm x (\bm x \models \alpha)$, but we find that for each entity pair combination, there exists $ \bm x $ that is more important than others. So we use the gold information of entity pairs to find the most important positive rule, and random sample some rules as a supplement. This significantly reduces the computational complexity.

